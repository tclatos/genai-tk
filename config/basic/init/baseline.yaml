
# A simple baseline configuration 
llm:
  list: ${paths.config}/providers/llm.yaml
  models:
    default: gpt_41mini_openai  
    fake: parrot_local_fake 
  cache: sqlite
  cache_path: data/llm_cache/langchain.db

embeddings:
  list: ${paths.config}/providers/embeddings.yaml
  models:
    default: ada_002_openai
    local: artic_22_ollama
    fake: embeddings_768_fake  # For testing
  cache: data/hf_models

# OLD configuration - still used by some code
vector_store:
  default: InMemory
  
embeddings_store:
  # Updated configuration using new field names (backend instead of id, storage instead of chroma_path)
  default:
    backend: InMemory
  
  in_memory_chroma:
    backend: Chroma
    embeddings: default
    table_name_prefix: embeddings
    config:
      storage: '::memory::'  # In-memory Chroma storage
  
  local_indexed:
    backend: Chroma
    embeddings: default
    table_name_prefix: embeddings
    record_manager: sqlite:///${paths.data_root}/vector_store/record_manager_cache.sql
    config:
      storage: ${paths.data_root}/vector_store  # Persistent storage on disk
  
  # Legacy configuration (deprecated - use backend/storage instead)
  # legacy_example:
  #   id: Chroma  # Use 'backend' instead
  #   config:
  #     chroma_path: /path/to/storage  # Use 'storage' instead


# Key-Value Stores configuration
kv_store: 
  default:
    type: LocalFileStore
    path: ${paths.data_root}/kv_store
    
  postgres: 
    type: SQLStore
    path: postgresql://${oc.env:POSTGRES_USER,unknown_user}:${oc.env:POSTGRES_PASSWORD,password}@localhost:5432/ekg
  
# LLM Cache configuration  
llm_cache:
  method: "memory"  # Use memory cache for tests