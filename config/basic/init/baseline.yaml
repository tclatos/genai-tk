
# A simple baseline configuration 
llm:
  list: ${paths.config}/providers/llm.yaml
  models:
    default: gpt_41mini@openai  
    fake: parrot_local@fake 
  cache: sqlite
  cache_path: data/llm_cache/langchain.db

embeddings:
  list: ${paths.config}/providers/embeddings.yaml
  models:
    default: ada_002@openai
    local: artic_22@ollama
    fake: embeddings_768@fake  # For testing
  cache: data/hf_models

# OLD configuration - still used by some code
vector_store:
  default: InMemory
  
embeddings_store:
  # Updated configuration using new field names (backend instead of id, storage instead of chroma_path)
 
  # default:
  #   backend: Chroma
  #   embeddings: default
  #   config:
  #     storage: ${paths.data_root}/vector_store  # Persistent storage on disk
  
  in_memory:
    backend: InMemory
  
  in_memory_chroma: &default_store  # Anchor is defined here first
    backend: Chroma
    embeddings: default
    table_name_prefix: embeddings
    config:
      storage: '::memory::'  # In-memory Chroma storage
  
  default: *default_store  # Now we can reference it
  
  chroma_indexed:
    backend: Chroma
    embeddings: default
    table_name_prefix: embeddings
    record_manager: sqlite:///${paths.data_root}/vector_store/record_manager_cache.sql
    config:
      storage: ${paths.data_root}/vector_store  # Persistent storage on disk
  


# Key-Value Stores configuration
kv_store: 
  default:
    type: LocalFileStore
    path: ${paths.data_root}/kv_store
    
  postgres: 
    type: SQLStore
    path: postgresql://${oc.env:POSTGRES_USER,unknown_user}:${oc.env:POSTGRES_PASSWORD,password}@localhost:5432/ekg
  
# LLM Cache configuration  
llm_cache:
  method: "memory"  # Use memory cache for tests

chains:   # Old stuff. Will likely be removed
  root: tests.chains
  modules:
    - ${..root}.joke 

structured:
  default:
    baml_client: tests.baml_client

monitoring: 
  langsmith : true
  project: GenAI_demo