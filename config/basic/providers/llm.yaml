########################
# LLM service provider
########################

# Registered LLMs Services

# LLM_id should follow Python variables constraints - ie no '-' no space etc
# Use pattern {self.model_family_name}_{version}
# model_name is provider specific.  It can contains several fields decoded in the factory.

# Capabilities can be : 'thinking', 'structured_outputs', 'pdf', 'vision'
# !!! THE ONE HERE ARE INCOMPLETE, AND POSSIBLY INCORRECT !!!! 

# Provider documentation links:
# OpenAI: https://platform.openai.com/docs/models
# DeepInfra: https://deepinfra.com/models
# Groq: https://console.groq.com/docs/models
# Together: https://docs.together.ai/docs/inference-models
# Google/Vertex AI: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text
# Ollama: https://ollama.ai/library
# EdenAI: :https//app.edenai.run/v2/feat/generative/chat ; https://app.edenai.run/bricks/text/chat  
# Azure OpenAI: https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
# OpenRouter: https://openrouter.ai/docs#models
# Amazon : https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html
# DeepSeek: https://api-docs.deepseek.com/quick_start/pricing 
# HuggingFace: https://huggingface.co/models
# LiteLLM: https://docs.litellm.ai/docs/providers
# ...

#cSpell: disable

llm:
  registry:
    - model_id: deepseek_v32
      providers:
        - deepseek: deepseek-chat
        - openrouter: deepseek/deepseek-v3.2
        - edenai: deepseek/deepseek-chat
      capabilities: ['structured_outputs', 'thinking']
      max_tokens: 8000
      context_window: 65536

    - model_id: parrot_local  # fake model for tests
      providers:
        - fake: parrot

    - model_id: gpt_oss120
      providers:
        - openrouter: openai/gpt-oss-120b:exacto
      capabilities: ['structured_outputs', 'thinking']
      max_tokens: 16384
      context_window: 200000

    - model_id: gpt_41mini
      providers:
        - openrouter: openai/gpt-4.1-mini
        - edenai: gpt-4.1-mini-2025-04-14
        - openai: gpt-4.1-mini-2025-04-14
      capabilities: ['structured_outputs', 'vision']
      max_tokens: 16384
      context_window: 1047576

    - model_id: qwen3_maxtinking
      providers:
        - openrouter: qwen/qwen3-max-thinking
      capabilities: ['thinking', 'structured_outputs']
      max_tokens: 16000
      context_window: 32768

    - model_id: mistral_7b
      providers:
        - custom:  # ChatOpenAI class parameters - see https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html
            model: mistralai/Mistral-7B-Instruct-v0.3
            base_url: https://gateway.cp4sc.platform.codex-platform.com/mistral/v1
            default_headers:
              X-Gravitee-Api-Key: ${oc.env:GRAVITEE_KEY, null } 
      capabilities: ['structured_outputs']
      max_tokens: 4096
      context_window: 32768

    - model_id: ministral_8b
      providers:
        - openrouter: mistralai/ministral-8b-latest
      capabilities: ['structured_outputs']
      max_tokens: 4096
      context_window: 131072

    - model_id: gpt_4o_mini
      providers:
        - litellm: openai/gpt-4o-mini
        - openai: gpt-4o-mini
      capabilities: ['structured_outputs', 'vision']
      max_tokens: 16384
      context_window: 128000

    - model_id: claude_haiku
      providers:
        - litellm: anthropic/claude-haiku
        - openrouter: anthropic/claude-haiku
      capabilities: ['structured_outputs']
      max_tokens: 4096
      context_window: 200000

    - model_id: google_gemini3flash
      providers:
        - litellm: google/gemini-3-flash
        - openrouter: google/gemini-3-flash
      capabilities: ['vision', 'structured_outputs']
      max_tokens: 8192
      context_window: 1000000